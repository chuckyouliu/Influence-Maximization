\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{courier}
\usepackage{multicol}
\usepackage{microtype} %required to fix courier linebreaking issues
\usepackage{hyperref} % make references into links
\usepackage{amsfonts}
\hypersetup{colorlinks=true, linkcolor=blue} %make them pretty links
\usepackage[all]{hypcap} %make them link to the right place
\graphicspath{{spark_images/}{png/}}

\begin{document}
	\title{AC290r: Influence Maximization Problem}
	\author{Stephen Carr and Charles Liu}
	\date{\today}
	\maketitle
	
	\section{Introduction}
	
	\subsection{Goal and Motivation}
	
	From a marketing perspective, social networking sites provide a large number of users with personal and persuasive information regarding products. With limited resources, one would want to know how to reach advertisement to the most number of people in this personalized manner. This problem can be reduced to a graph, where each user is a node and each relationship between users is represented by an edge with some weight to indicate the strength of their relationship. Which user, or set of users, has the ability to spread your product to the greatest number of potential customers?
	
	\subsection{Data set}
	
	The user network of the online restaurant reviewing site Yelp is used in this study. Each account represents the node, and the edges are to be generated by looking at which accounts posted reviews of the same restaurant within a certain time period of one another. This information (the user, their total number of reviews, and how many reviews they have written within 1 month of another user) was pulled from the Yelp site, turned into a library format, and then imported into a python development environment using NetworkX. 
	
	For most of the prototyping of a solution, a subset of the North Carolina network was taken (NC mini), which consisted of 240 nodes. We performed further testing of algorithms, a full North Carolina network was used (NC full). The NC full network consisted of 13665 weakly connected nodes (i.e. the graph is connected if we ignore the directionality of the edges). It has a strongly connected component of 9361 nodes (subset of the graph where you can reach any node from any other node). Depending on the directionality of the edges for the weakly connected nodes, we expect the maximum possible solution to be between 9361 and 13665 (the maximum seems to be 11517, see section 4.2). Our goal was to find a solution for the total United States network, but this data set is extremely large (about 350,000 nodes).
	
	\subsection{Independent Cascade, $I(s)$}
	
	We start with a set of nodes $s = \{n_1,n_2,...,n_k\}$ and wish to find the number of nodes which are activated through our probabilistic network of influence. So for each node $n$ in our original set $s$, we look at all nodes connected to $n$ by an edge and use the beta distribution to determine if the connected node $n_c$ activates (we ignore $n_c$ if it is already activated). The beta distribution $\boldsymbol{B}$ is given by:
	\begin{equation}
	\boxed{
			\int_{-\infty}^{\infty} \frac{dx}{\boldsymbol{B}(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta - 1} = 1
		}
	\end{equation}
	Where $\beta$ is taken to be the number of reviews node $n_c$ has written within one month of node $n$ writing a review of the same restaurant, and $\alpha$ is taken to be the number of reviews $n_c$ has written. A uniform distribution is taken between $0$ and $1$ and if its value is less than $\boldsymbol{B}(\alpha,\beta)$ then $n_c$ activates and we repeat the same process for neighbors of $n_c$. This recursive process continues until either all nodes are activated or no activated nodes are left in the loop. The number of nodes activated is called $I(s)$.
	
	
	\section{Stochasticity}
	
	\subsection{Averaged Trial Function, $f_N(s)$}
	
	Since each step of the independent cascade is a random sampling of the beta distribution, there is no guarantee that two different measurements of $I(s)$ will be the same, or even "close" to one another, as seen in \ref{fig:Inf_1000}. 
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=100mm]{N1000_influence.png}
		\caption{Histogram of values $I(s)$ over 1000 iterations for a fixed node in the 240 North Carolina subgraph.}
		\label{fig:Inf_1000}
	\end{figure}
	
	Because of this, we must define a new measurement of a node's influence, namely its mean over $N$ samples:
		\begin{equation}
		\boxed{
			f_N(s) = \frac{1}{N}\sum_1^N I(s)
		}
		\end{equation}
		
		
		
	\subsection{Properties of $f_N(s)$}
	Obviously we cannot take $N$ arbitrarily large, so we must understand how $f_N$'s properties vary with $N$. Taking a 240 node subset of North Carolina, we obtain the following relationship between $N$ and the standard deviation of $f_N$ in \ref{fig:lambda}.
	
		\begin{figure}[h!]
			\centering
			\includegraphics[width=100mm]{lambda_regression.png}
			\caption{Fitted regression for the standard deviation of $f_N(s)$. This was with 100 trials for each N value. The blue line is the simulation, while the red line is the fitted relationship.}
			\label{fig:lambda}
		\end{figure}
			
	
	\section{Implementation}
	
	\subsection{Greedy Algorithm}
	Although this problem is inherently non-linear in its parameters (the starting set of nodes), we can try to solve it in a linear way. This is called the "Greedy Algorithm":
	Given nodes $s_i = \{n_1, n_2,..., n_i\}$, check every remaining node to find node ${n_{i+1}}$ that maximizes $f(s_{i+1})$ where $s_{i+1} =  \{n_1, n_2, ..., n_i, n_{i+1}\}$.
		
	
	\subsection{MCMC and Simulated Annealing}
	For a large problem with $N$ nodes, the greedy algorithm would need to check roughly $N^k$ node arrangements (first $N$, then $N-1$, etc.). For $k > 2$ this will not be feasible. Instead, we implement a Monte Carlo method of picking sets of nodes at random and evaluating their influence. Although this method will not guarantee we find the optimal solution, we can define a iterative method (Markov Chain) that on average will find a better solution after each step.
		
	The idea of a Markov Chain Monte Carlo (MCMC) algorithm is to use the function of interest to assign a weight to every position in parameter space, and use this weight to add control to the "random steps" of a Monte Carlo method. Our function is the mean influence, $f_N(s)$, and our parameter space is the unordered combination of $k$ nodes $s = \{n_i\}$. Instead of always taking a "random step" around parameter space, we assign to each step a probability of success:
		
		\begin{equation}
		\boxed{
			P(s_1,s_2) = e^\frac{-\Delta E}{T}, \Delta E(f(s_2),f(s_1))
		}
		\end{equation}
		
		Where $s_1$ is our current set of nodes and $s_2$ is a new proposed set. Controlling the Monte Carlo method in this manner is known as Simulated Annealing, as the probability formula mimics the Boltzmann factor of statistical mechanics. The Boltzmann factor describes the relative probability of a particle being in state $s$ when it is in a thermal equilibrium of temperature $T$. Annealing such a system means heating it up and then slowly cooling it down, allowing the particle(s) to settle into a new arrangement, hopefully of lower energy. For our problem we take the "Energy" to be $-f_N(s)$, since we want to maximize $f_N(s)$. 
		
		The simplest form for $\Delta E$ is $f(s_1) - f(s_2)$. In this case, if $f(s_2) > f(s_1)$ we take $s_2$ with probability 1, otherwise there is a chance of taking $s_2$ even though its estimated influence is less than our current state. This gives us an additional parameter to control our MCMC method: the "Temperature" $T$ controls how likely we are to take a step away from a maximum. So when we are not "near" a local maximum, a high $T$ allows us to sweep the parameter space quickly, but as we near a solution we take a low $T$ to make the MCMC very sensitive to small changes in $f_N(s)$.
	
	\subsection{Advantages and Shortcomings}
	
	As the MCMC is an iterative process that gives a solution from the first step that slowly improves with each iteration, it is better than the greedy algorithm which needs to finish in its entirety before an answer can be output. Furthermore, the greedy algorithm does not guarantee to find the answer, and it should find the same sub-optimal solution everytime (provided $N$ is large enough). The MCMC will "eventually" find the global maximum influence, as it will eventually cover all possible permutations of nodes. So for large graphs, the MCMC method is clearly superior, so the only work left to do is to find how to speed up each iteration without sacrificing accuracy. A number of methods to speed up the MCMC process is covered next.
	
	\subsection{Choice of MCMC Parameters for Speed}
	
	\subsubsection{$\Delta$ E, N, and T}
	
	For small graphs, $\Delta E = f(s_1) - f(s_2)$ will work. But for large graphs, this will allow $P(s_1,s_2)$ to be unsuitable large or small. Instead we take $\Delta E = log(f(s_1)) - log(f(s_2))$, allowing $P(s_1,s_2)$ to be tractable both conceptually and computationally.
	
	
	
	We have already established that $\sigma_{f_N}$ goes like $N^{-1/2}$. For both the small and large subgraphs of North Carolina, $N > 1000$ seems to yield $\sigma << 1$.
	
	
	
	Start with $T = 1$. After each successful step, set $T_{n+1} = \alpha*T_n$ where $\alpha < 1$.

	\subsubsection{Allowed Nodes $(s_i)$}
	
	Randomly swap one of the nodes of $s_n$ to any other node to generate $s_{n+1}$. This was chosen because we found for large graphs that there were strongly-connected sub-graphs. Taking nodes nearby your current nodes therefore had little to no effect on $f_N$, as most "neighbors" were still within this subgraph (this is elaborated upon in the section 4.2.1).
	
	We also added "swap annealing" to our MCMC process, the probability of taking nodes $n_2$ to replace node $n_1$:
	
	\begin{equation}
	\boxed{P_e(n_2,n_1) = e^\frac{N_e(n_2) - N_e(n_1)}{T_e}
	}
	\end{equation}
	
	$N_e(n)$ is the logarithm of the number of edges of node $n$, and $T_e$ is a separate simulated annealing temperature. This makes it less likely for us to calculate $f_N$ for a node with a low number of edges, thus reducing the amount of time spent evaluating "bad" solutions.
	
	\section{Results}
	
	
	\subsection{NC\_Mini}
	
	\subsubsection{Greedy Algorithm}
	
	For $k = 3$ on our 240 node North Carolina subset we found variation in the maximal $I(s)$.
	
	\begin{table}[h!]
		\begin{center}
			\caption{100 runs with k=3, t=10}
			\label{tab:table1}
			\begin{tabular}{|c|c|c|c|} 
				\hline
				N & min & max & $\lambda$\\
				100  & 42.98 & 51.16 & 0.840\\
				300  & 44.73 & 49.24 & 0.908\\
				500  & 45.04 & 47.696 & 0.944\\
				1000 & 45.66 & 47.31 & 0.965\\
				\hline
			\end{tabular}
		\end{center}
	\end{table}
	
	{\centering
		\hspace*{-3.75cm}
		\includegraphics[scale=0.55]{100_scatter}
		\includegraphics[scale=0.55]{300_scatter}
		\hspace*{-3.75cm}
		\vspace*{1.25cm}
		\includegraphics[scale=0.55]{500_scatter}
		\includegraphics[scale=0.55]{1000_scatter}	
		\hspace*{-3.75cm}
		\includegraphics[scale=0.4]{100_pie}
		\includegraphics[scale=0.4]{300_pie}
		\hspace*{-3.75cm}
		\includegraphics[scale=0.4]{500_pie}
		\includegraphics[scale=0.4]{1000_pie}
		\hspace*{-3.75cm}
		\par}
	
	
	The scatterplots of the max values over the 100 trials show the convergence of results as N increases. We can similarly see the convergence through the number of nodes selected over the course of our trials. More formally, let $\mathbb{N}_i$ represent the set of nodes returned over all 100 trials of $N=i$. We have
	
	\begin{equation}
		\boxed{
			\mathbb{N}_i \subset \mathbb{N}_j, i > j\\
		}
	\end{equation}
	
	\subsubsection{MCMC}
	MCMC finds answers of similar quality. A maximum influence of 46.99 with 2 out of 3 of the nodes from the greedy algorithm!
		\begin{figure}[h!]
			\centering
			\includegraphics[width=70mm]{mcmc_mini.png}
			\caption{MCMC on NC mini with swap annealing, N = 128.}
			\label{fig:mcmc_mini}
		\end{figure} 
		
	
	
	\subsection{NC\_Full}
	
	Running a cascade from one node but making each edge always activate (instead of a probabilistic activation) yields a maximum influence of 11517. The strongly connected component of this graph is 9361, so its unlikely that 11517 can be reached as the remaining $\approx$ 2000 nodes are connected to the strongly connected graph by one to four low probability edges.
	
	To understand the properties of this graph, let us look at a histogram of $I(s)$ for the 3 nodes with the most number of edges (See heuristic solution below). This solution almost always will return an answer within 150 of the mean ($f_N \approx 8960$, $\sigma \approx 1\%$), showing a normal distribution (Figure \ref{fig:naive}).
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=80mm]{NC_full_solution_hist.png}
		\caption{Histogram of values $I(s)$ over 100 iterations for the heuristic solution of the full North Carolina graph.}
		\label{fig:naive}
	\end{figure} 
	
	

	\subsubsection{Greedy Algorithm}
	
	We wanted to do the same analysis as with $N$ with respect to the depth of search. For the larger NC\_full graph, our results were:
	
	\begin{table}[h!]
		\begin{center}
			\caption{15 runs with k=3, N=50}
			\label{tab:table2}
			\begin{tabular}{|c|c|c|c|} 
				\hline
				t & min & max & $\lambda$\\
				1  & 1465.32 & 1504.28 & 0.9741\\
				4  & 8453.82 & 8480.96 & 0.9968\\
				7  & 8966.66 & 8980.06 & 0.9985\\
				10 & 8985.16 & 8998.66 & 0.9985\\
				\hline
			\end{tabular}
		\end{center}
	\end{table}
	
	{\centering
		\hspace*{-3.75cm}
		\includegraphics[scale=0.4]{full_1t}
		\hspace*{1cm}
		\includegraphics[scale=0.4]{full_4t}
		\hspace*{-3.75cm}
		\includegraphics[scale=0.4]{full_7t}
		\hspace*{2cm}
		\includegraphics[scale=0.4]{full_10t}
		\par}	
	
	Due to computational limitations, we were only able to run 15 trials for each depth. However, we were still able to see some interesting patterns. First as a sanity check, we saw the influence function return higher values as depth increased - this of course should be the case. Second, we see $\lambda$ increase as depth increases for a consistent $N$. From $t=7$ to $t=10$ this seems to plateau, which may point to some bounds on the diameter of the largest connected components in the graph. Finally, unlike with increasing $N$, we see that the set of nodes returned over the trials increases as depth increases. In fact for $t=10$, 45 different nodes returned meaning there was a different node in every result.
	
	What this implies is that there is a large connected component within NC\_full that, regardless of the nodes chosen, will give you roughly the optimal number from the greedy algorithm. We wrote a function to calculate the number of connected components from the set of nodes returned and found such:
	
	\begin{table}[h!]
		\begin{center}
			\caption{Connected Components for t}
			\label{tab:table3}
			\begin{tabular}{|c|c|c|c|} 
				\hline
				t & Total Nodes & Connected Components & Max Component Size\\
				1  & 5 & 1 & 5\\
				4  & 6 & 1 & 6\\
				7  & 35 & 6 & 30\\
				10 & 45 & 8 & 38\\
				\hline
			\end{tabular}
		\end{center}
	\end{table}
	
	In $t=7,10$ there is a large component where the majority of max nodes are from, and then the rest are 1-node components. Given that we know this information now, we can smartly choose three nodes that cover different components and run on a larger $N$. We should also beat the heuristic implementation of choosing the three nodes with the most edges, because as it turns out those three nodes are all in the same large component.
	
	For the chosen nodes from different components, we want to choose the two nodes with the most edges in our set of 8 in the $t=10$ run. It turns out that every node in the other components had only 1 edge, so we choose two at random.
	
	\begin{table}[h!]
		\begin{center}
			\caption{Heuristic vs. Best Components Mix on N=1000}
			\label{tab:table4}
			\begin{tabular}{|c|c|} 
				\hline
				Nodes & Mean\\
				Heuristic & 8960.0698 \\
				Component Mix & 8970.6878 \\
				\hline
			\end{tabular}
		\end{center}
	\end{table}
	
	Heuristic nodes: 
	\begin{center}
		[CvMVd31cnTfzMUsHDXm4zQ, \\
		NzWLMPvbEval0OVg\_YDn4g,\\
		4G68oLRY3aHE5XUt\_MUUcA]\\
	\end{center}
	
	Component mix: 
	\begin{center}
		[wadTky-Lw9MVqDhiJUqPJA,\\
		OYrEBnzYbL3dkoVdy1C10Q,\\
		CvMVd31cnTfzMUsHDXm4zQ]\\
	\end{center}
	
	We have a lower value than our max from the $t=10$ run, which is expected as we saw a similar convergence in the NC\_mini tests. We do in fact see a better result from our component mix than the heuristic, though still within a standard deviation. The other components aren't heavily connected so adding nodes from them have very little benefit in the NC\_Full case, but perhaps could be much more useful for the full US graph.
	
	
	\subsubsection{MCMC}
	
	
	Running our MCMC algorithm without annealed swapping found many good solutions of $\approx$ 9000 influence (Figure \ref{fig:mcmc_noswap}). However we see that the MCMC process is very unstable here because the "temperature" is not yet low enough over 100 iterations to ensure we stay near our maximum solution.
	
			\begin{figure}[h!]
				\centering
				\includegraphics[scale=0.7]{nc_full_mcmc.png}
				\caption{Random swapping method for MCMC of 3 nodes for the NC full graph. N = 1024}
				\label{fig:mcmc_noswap}
			\end{figure}
			
	Using the annealed swapping method gives a much more stable MCMC process. Even without setting the temperature much below $T = 1$, this method found a $f_N \approx 9000$ solution within 3 steps, and then never went below $f_N \approx 8960$ for over 100 steps (again, N = 1024). It seems then that annealed swapping may improve the efficiency of a large scale MCMC as it will prevent the algorithm from having to calculate clearly inferior node selections (i.e. replacing a node with 1000 edges with one with 5). Furthermore, it will only impact the quality of the search in bizarre edge cases. For example, if there is a node with 10 edges of probability 1 to high edge-count nodes. Finding this node will be unlikely, but not impossible with the annealed swap.
		
		This run gave a max influence of 8975.7 with nodes
		\texttt{[9ki\_-I7oW6YxnUZBCmxs5Q, GllP-lYGO1m61G4ly5fYTQ, iCePpGSAgAOSYZ9TkCQ09w]}
		
	\subsection{US Graph}
			
			Using what we learned from our experiments on NC\_mini and NC\_full, we test four different algorithms for finding the most influential users.
			
			\begin{description}
				\item[Heuristic Algorithm] \hfill \\
				This is the standard heuristic algorithm that specifies the three nodes with the most edges. We found with N=100, t=10:
				
				Nodes = \begin{center}
					[Iu3Jo9ROp2IWC9FwtWOaUQ,\\
					OaFcpi3W4AwxrD8W2pgC\_A,\\
					glRXVWWD6x1EZKfjJawTOg]\\
				\end{center}
				
				Influence = 184144.69
				
				\item[Heuristic Algorithm V2] \hfill \\
				Given the possibility of a heavily connected component, we can adjust our heuristic algorithm to choose the node with the most edges, remove all connected nodes to that edge, and reiterate until we have 3 nodes. We found with N=100, t=10:
				
				Nodes = \begin{center}
					[Iu3Jo9ROp2IWC9FwtWOaUQ,\\
					2rghC4XAXTXuY\_ZGocVB1g,\\
					znSMP7mggycgF-NP1rz93g]\\
				\end{center}
				
				Influence = 184198.57
				
				\item[Greedy Algorithm] \hfill \\
				To account for the size of the US graph, we devise a greedy algorithm that checks the connected components and iterates on $N$ and $t$. We define the following variables:
				
				\begin{tabular}{|c|c|} 
					\hline
					$N_0$ & Initial N \\
					$t_0$ & Initial t \\
					$I_N$ & Increment for N \\
					$I_t$ & Increment for t \\
					$S$ & Set of nodes \\
					$K$ & Set of selected nodes \\
					$L$ & Number of trials to run\\
					\hline
				\end{tabular}	
				
				\begin{enumerate}
					\item Initialize $N=N_0,t=t_0,S=\text{all nodes},K=\emptyset$
					\item Run greedy algorithm: Cascade on $\{x\} \cup K, \forall x \in S$ with specified $N$ and $t$ and return set of max nodes $X$ over the $L$ trials
					\item Divide $X$ into connected components, select node $n$ with most edges in most populated connected component
					\item $K = K \cup [n]$
					\item $S = S \setminus \{\text{Nodes connected to }n\}$
					\item $N = N + I_N$
					\item $t = t + I_t$
					\item Repeat steps 2-7 until $|K|=k$
				\end{enumerate}
				
				\item[MCMC] \hfill \\
				
				The MCMC method outlined for the NC full graph should also work for the full US graph. Implementing the annealed swapping will still increase the efficiency of the algorithm. 
				
				\end{description}
				
				As of \today, we have not been able to run either the Greedy or MCMC algorithms on the full US graph, as running $f_N$ with $N = 100$ for each step of our algorithm is outside the scope of our current computational resources. 
				
			
			\subsection{Future steps}
			
			The initial and incremental $N$ and $t$ in the greedy algorithm can be calibrated. It's effect will be a tradeoff of the number of max nodes returned (and therefore the possible solution set) and the runtime of the algorithm.
			
			\subsection{\href{https://github.com/chuckyouliu/Influence-Maximization}{Source Code}}
			
			*Section header is a link to github
			
			\begin{description}
				\item[Spark\_N.ipynb] \hfill \\
				NC\_mini runs for N=100, 300, 500, 1000
				\item[Greedy-Full] \hfill \\
				NC\_full runs on Azure
				\item[Greedy\_Analysis] \hfill \\
				Compilation and Analysis of test runs
				\item[sparkruns] \hfill \\
				Folder of json's of all tests
				\item[mcmc\_azure] \hfill \\
				Folder of MCMC runs
			\end{description}
	
	
\end{document}